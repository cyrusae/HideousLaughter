---
title: "PLS scratchpad 3"
output: html_notebook
---

```{r setup}
devtools::load_all()
```
**What now?**
- Want to unify zip file retrieval, csv extraction, and initial csv sanitization
- Want to be able to delete unneeded files (consider temp directory? this depends a lot on what I want the failure state to be?)
```{r}
pls <- get_pls_urls()
```
```{r}
get_pls_csv <- \(file, response, fy, here) {
  assertthat::is.readable(file) #is the file readable?
  assertthat::is.writeable(here::here(here)) #is the destination writable?
  assertthat::is.string(fy) #is the input fy coherent?
  assertthat::is.string(response) #do we know what the response is?
  dest <- paste0(here::here(here), '/pls_', 
                 response, '_', fy, '.csv')
  dt <- data.table::fread(file = file)
  dt[dt == -9] <- NA #Remove suppressed data
  dt[dt == -4] <- NA #Remove for closures
  dt[dt == -3] <- NA #Remove for closures
  dt[dt == -1] <- NA #Remove unanswered questions
  dt[dt == 'M'] <- NA #Remove missing values
  if ('MICROF' %in% names(dt)) dt[MICROF == 'N', MICROF := NA] #NA for the MICROF field only
  if ('RSTATUS' %in% names(dt)) dt[RSTATUS == 3, RSTATUS := NA] #remove nonrespondents 
  data.table::fwrite(dt, file = dest)
  assertthat::is.readable(path = dest)
  dest #return successfully-written file
}
```

```{r}
get_pls_data <- \(url, extract = 'fy20..', 
                  here = 'data/raw/PLS_csvs') {
  assertthat::is.string(url) #Did we get a URL that we can ping
  assertthat::is.string(here) #Is the desired path viable
  if (is.null(names(url))) {
    fy <- stringr::str_extract(url, extract)
    assertthat::assert_that(length(fy) == length(extract))
    names(url) <- fy
  }
  fy <- names(url) #Get the FY value out
  fp <- paste0(here::here(here), '/', fy)
  if (!dir.exists(fp)) dir.create(fp) #Create directory if needed
    #Note: still deciding whether we're keeping the zip files and deleting everything else or what
    #TODO Add error handling once we're sure of cleanup steps 
  assertthat::is.writeable(fp) #Can we write to the directory
  zipfile <- paste0(fp, '/', fy, '.zip')
  if (!file.exists(zipfile)) {
    download.file(url = url, destfile = zipfile, quiet = TRUE)
  } #Download a file if it doesn't appear to exist yet 
  assertthat::is.readable(zipfile) #Did we get the zip successfully (or have it already)?
  zip_contents <- grep('\\w+\\.csv$', #find only the CSV files
                       unzip(zipfile = zipfile, 
                             list = TRUE)$Name, #Names only
                       ignore.case = TRUE, value = TRUE)
  unzip(zipfile = zipfile, files = zip_contents,
        exdir = fp) #put the CSV files in the /fy20XX/ directory
  zip_contents <- list.files(path = fp, pattern = '\\w+\\.csv$',
                             full.names = TRUE, recursive = TRUE,
                             include.dirs = TRUE)
  assertthat::assert_that(length(zip_contents) == 3) #make sure there are specifically three CSV files here
  zip_nrows <- check_nrows(files = zip_contents)
  zip_results <- data.table::data.table(
    path = zip_contents,
    nrows = zip_nrows
  ) #track features about the files that we'll need in a bit
  zip_results <- zip_results[nrows != min(zip_results$nrows), ] #remove the states file from consideration 
  zip_results[nrows == max(zip_results$nrows),
              response := 'outlet'] #largest file will be outlets
  zip_results[nrows == min(zip_results$nrows), 
              response := 'admin'] #remaining will be administrative entities 
  csvs <- furrr::future_map2_chr(.x = zip_results$path,
                                 .y = zip_results$response,
                                 .f = get_pls_csv,
                                 fy = fy, here = here)
  # Clean up
  process_files <- list.files(path = fp, full.names = TRUE) %>%
    setdiff(zipfile) #Get everything but the original zip
  unlink(process_files, recursive = TRUE) #Delete 
  csvs #return paths to created files
}
```

Above works! 

Do we want `purrr` or `furrr`? (Also, how to avoid duplicating labor? I guess I might just end up leaving that up to `targets` given how I've written it thus far...)

- Thought: Could we check the main `here` input for available FYs and exclude any by that name from list processing by default? That sounds like a good top-level way of handling it 

TODO:
- Add wrapper to check for existing applicable CSVs matching the pattern 
- Make default option of not overwriting/optional overwriting
- Test both
- There are more things to move to `pls.R` self 

```{r}
microbenchmark::microbenchmark(
  purrr::map(.x = pls, .f = get_pls_data), 
  furrr::future_map(.x = pls, .f = get_pls_data), times =  10)
```
`furrr` it is! (both min <10s but `purrr` max is <885s and `furrr` max is <30s so like no contest)

```{r}
profvis::profvis(get_pls_data(pls[1]))
```
```{r}
profvis::profvis(furrr::future_map(.x = pls, .f = get_pls_data))
```

Well, the good news is that there's no obviously visible bottleneck here and the bad news is also that there's no obviously visible bottleneck here. Moving on it is

```{r}
profvis::profvis(get_pls_urls())
microbenchmark::microbenchmark(get_pls_urls())
```
N.B. the URL retrieval itself takes between a quarter of a second and a second and a half, so that's okay
