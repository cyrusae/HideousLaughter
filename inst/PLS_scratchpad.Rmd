---
title: "PLS scratchpad"
output: html_notebook
---

CLEANUP 
- Prefix `base::` etc.
- Make sure all packages are prefixed
- Label steps
- Clean up after yourself
- Variable names that make sense 
- Consistent file paths

REFACTOR
- `furrr`?
- iterate over years instead of over file types
- be selective about which files are extracted
- retain identity of FYs 
- eliminate for loops 
- use `here` for file paths 
- use functions 
- delete unnecessary files
- consider being `polite` here 

REFACTOR x2: WHEN IT WORKS ALL THE WAY THROUGH
- use `targets`
- audit dependencies
- move dependencies to package project as a whole 

REFACTOR x3: MOVE TO CANONICAL PACKAGE FORM
- add unit tests 

DOCUMENT
- explain process
- explain dependencies
- explain potential pitfalls (e.g., this should be resilient against new years of PLS data, but if they redesign their site we get to be sad)
- consistent coding style!!! (magrittr pipe over native I think...)
- scratchpad files list 

CONTEMPLATE
- what does unit testing for data integrity look like
- toolkit for generating PLS data should exist but also clean PLS data should be pre-packaged 
- get external reality checks for accessibility to third parties 

```{r}
require(rvest) #scrape from IMLS
require(data.table) #handle CSVs 
require(purrr) #do multiple things at once
require(furrr) #same 
```

### retrieve files 

```{r}
pls_url <- 'https://www.imls.gov/research-evaluation/data-collection/public-libraries-survey'
```
```{r}
# s <- session(pls_url)
# pg <- s |> rvest::read_html()
  # rvest::session_follow_link(css = 'ui-accordion-content')
# pls_url |> rvest::read_html() |> rvest::html_nodes(xpath = "//article//div[@data-ui-role]//div")
```
```{r}
#downloaded the page to stop having to send them requests
#should work against actual url
pg <- rvest::read_html('./data/raw/PLS_testpage.html') 
pls_divs <- pg %>% 
  rvest::html_elements('.ui-accordion-content') %>%
  rvest::html_children() %>%
  rvest::html_element('a')
pls_list <- rvest::html_attrs(pls_divs)
pls_list <- pls_list[grepl('*pls_fy', pls_list)]
m <- regexpr('fy20..', pls_list)
n <- substring(pls_list, m) 
n <- substr(n, 1, 6) #strip filenames down to "fy20XX"
for (i in seq_along(pls_list)) {
  download.file(pls_list[[i]], paste0('data/raw/PLS_csvs/', n[i], '.zip')) ##TODO check working directory congruence here, if it doesn't run twice that's your problem 
  #creates a sequence of "fy20XX.zip" files
}
```

TODO: Make sure the above works live as well 


### extract files 

```{r}
pls_dir <- '../data/raw/PLS_csvs/'
z <- list.files(path = pls_dir, pattern = '*.zip', full.names = T) #get list of zips 
```

```{r}
purrr::map(.x = z, .f = unzip, exdir = pls_dir) #extract all zips
# FY2014, 2015, and 2017 are weird, because we can't have nice things
## TODO: extract only CSVs; make the zips with nested folders special cased or not terrible?
subfolders <- list.dirs(path = pls_dir)[-1] #here's our problems
```
```{r}
subfiles <- purrr::map(.x = subfolders, .f = list.files,
                       pattern = '*.csv', full.names = TRUE)
#here's their contents
```

Process for getting desirable files out of this:
- list location of all csv files
- get their corresponding fiscal year (regex again)

Here's our function wrangling logic
- want row count for each of these files 
- group by FY 
- drop lowest rowcount 
- rename highest to outlets
- rename middle to admin entities

TODO: Refactor this to iterate over years instead of doing it the hard way once it's working, or possibly instead of digging myself in deeper if I hit a wall 

WISHLIST: Would like to not duplicate as much labor identifying FYs 

---

2023.03.27, night: Okay yes I think the responsible thing to do is stop and back up to year-by-year handling. Although... `subfiles` is free batched test for doing that, I suppose.

### process year-on-year `subfiles` 
